{"env_info": "sys.platform: linux\nPython: 3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA GeForce RTX 3090\nCUDA_HOME: None\nGCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\nPyTorch: 1.8.1+cu111\nPyTorch compiling details: PyTorch built with:\n  - GCC 7.3\n  - C++ Version: 201402\n  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n  - Intel(R) MKL-DNN v1.7.0 (Git Hash 7aed236906b1f7a05c0917e5257a1af05e9ff683)\n  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n  - NNPACK is enabled\n  - CPU capability usage: AVX2\n  - CUDA Runtime 11.1\n  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86\n  - CuDNN 8.0.5\n  - Magma 2.5.2\n  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.8.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, \n\nTorchVision: 0.9.1+cu111\nOpenCV: 4.6.0\nMMCV: 1.4.0\nMMCV Compiler: GCC 7.3\nMMCV CUDA Compiler: 11.1\nMMDetection: 2.19.0+276ecbf", "config": "optimizer = dict(\n    type='SGD', lr=0.30000000000000004, momentum=0.9, weight_decay=4e-05)\noptimizer_config = dict(grad_clip=None)\nlr_config = dict(\n    policy='CosineAnnealing',\n    min_lr=0,\n    warmup='linear',\n    warmup_iters=300,\n    warmup_ratio=0.1)\nrunner = dict(type='EpochBasedRunner', max_epochs=300)\ncheckpoint_config = dict(interval=10)\nlog_config = dict(\n    interval=20,\n    hooks=[dict(type='TextLoggerHook'),\n           dict(type='TensorboardLoggerHook')])\ncustom_hooks = [\n    dict(type='NumClassCheckHook'),\n    dict(type='CycleEMAHook', cycle_epoch=40, resume_from=None, priority=49)\n]\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nload_from = '/home/aiteam/tykim/scratch/lightweight/mmdet_nano/checkpoints/picodet_l_640_40.4.pth'\nresume_from = None\nworkflow = [('train', 1)]\nmodel = dict(\n    type='PicoDet',\n    backbone=dict(\n        type='ESNet',\n        model_size='l',\n        out_indices=[2, 9, 12],\n        frozen_stages=-1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        norm_eval=False,\n        act_cfg=dict(type='HSwish'),\n        se_cfg=dict(\n            conv_cfg=None,\n            ratio=4,\n            act_cfg=({\n                'type': 'ReLU'\n            }, {\n                'type': 'HSigmoid',\n                'bias': 3.0,\n                'divisor': 6.0,\n                'max_value': 6.0\n            })),\n        init_cfg=dict(\n            type='Pretrained',\n            checkpoint=\n            '/home/aiteam/tykim/scratch/lightweight/mmdet_nano/checkpoints/ESNet_x1_25_pretrained_mmdet_format.pth'\n        )),\n    neck=dict(\n        type='CSPPAN',\n        in_channels=[160, 320, 640],\n        act_cfg=dict(type='HSwish'),\n        norm_cfg=dict(type='BN', requires_grad=True),\n        out_channels=160,\n        num_features=4,\n        expansion=1,\n        num_csp_blocks=1),\n    bbox_head=dict(\n        type='PicoDetHead',\n        num_classes=1,\n        in_channels=160,\n        feat_channels=160,\n        stacked_convs=4,\n        kernel_size=5,\n        share_cls_reg=True,\n        use_depthwise=True,\n        reg_max=7,\n        act_cfg=dict(type='HSwish'),\n        strides=[8, 16, 32, 64],\n        norm_cfg=dict(type='BN', requires_grad=True),\n        use_vfl=True,\n        loss_cls=dict(\n            type='VarifocalLoss',\n            use_sigmoid=True,\n            alpha=0.75,\n            gamma=2.0,\n            iou_weighted=True,\n            loss_weight=1.0),\n        loss_dfl=dict(type='DistributionFocalLoss', loss_weight=0.25),\n        loss_bbox=dict(type='GIoULoss', loss_weight=2.0)),\n    train_cfg=dict(\n        assigner=dict(\n            type='SimOTAAssigner', num_classes=1, use_vfl=True, iou_weight=6),\n        allowed_border=-1,\n        pos_weight=-1,\n        debug=False),\n    test_cfg=dict(\n        nms_pre=1000,\n        min_bbox_size=0,\n        score_thr=0.025,\n        nms=dict(type='nms', iou_threshold=0.6),\n        max_per_img=100))\ndata_root = '/home/aiteam/tykim/scratch/lightweight/mmcv_phone/hold_smartphone'\ndataset_type = 'COCODataset'\nimg_scales = [(576, 576), (608, 608), (640, 640), (672, 672), (704, 704)]\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile', to_float32=True),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(type='MinIoURandomCrop'),\n    dict(\n        type='Resize',\n        img_scale=[(576, 576), (608, 608), (640, 640), (672, 672), (704, 704)],\n        multiscale_mode='value',\n        keep_ratio=False),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(\n        type='PhotoMetricDistortion',\n        brightness_delta=32,\n        contrast_range=(0.5, 1.5),\n        saturation_range=(0.5, 1.5),\n        hue_delta=18),\n    dict(\n        type='Normalize',\n        mean=[123.675, 116.28, 103.53],\n        std=[58.395, 57.12, 57.375],\n        to_rgb=True),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile', to_float32=True),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(640, 640),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=False),\n            dict(type='RandomFlip'),\n            dict(\n                type='Normalize',\n                mean=[123.675, 116.28, 103.53],\n                std=[58.395, 57.12, 57.375],\n                to_rgb=True),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img'])\n        ])\n]\ndata = dict(\n    samples_per_gpu=16,\n    workers_per_gpu=4,\n    persistent_workers=True,\n    train=dict(\n        type='CocoDataset',\n        ann_file=\n        '/home/aiteam/tykim/scratch/lightweight/mmcv_phone/hold_smartphone/annos/train.json',\n        img_prefix=\n        '/home/aiteam/tykim/scratch/lightweight/mmcv_phone/hold_smartphone/imgs',\n        pipeline=[\n            dict(type='LoadImageFromFile', to_float32=True),\n            dict(type='LoadAnnotations', with_bbox=True),\n            dict(type='MinIoURandomCrop'),\n            dict(\n                type='Resize',\n                img_scale=[(576, 576), (608, 608), (640, 640), (672, 672),\n                           (704, 704)],\n                multiscale_mode='value',\n                keep_ratio=False),\n            dict(type='RandomFlip', flip_ratio=0.5),\n            dict(\n                type='PhotoMetricDistortion',\n                brightness_delta=32,\n                contrast_range=(0.5, 1.5),\n                saturation_range=(0.5, 1.5),\n                hue_delta=18),\n            dict(\n                type='Normalize',\n                mean=[123.675, 116.28, 103.53],\n                std=[58.395, 57.12, 57.375],\n                to_rgb=True),\n            dict(type='Pad', size_divisor=32),\n            dict(type='DefaultFormatBundle'),\n            dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])\n        ],\n        classes=('smartphone', )),\n    val=dict(\n        type='CocoDataset',\n        ann_file=\n        '/home/aiteam/tykim/scratch/lightweight/mmcv_phone/hold_smartphone/annos/val.json',\n        img_prefix=\n        '/home/aiteam/tykim/scratch/lightweight/mmcv_phone/hold_smartphone/imgs',\n        pipeline=[\n            dict(type='LoadImageFromFile', to_float32=True),\n            dict(\n                type='MultiScaleFlipAug',\n                img_scale=(640, 640),\n                flip=False,\n                transforms=[\n                    dict(type='Resize', keep_ratio=False),\n                    dict(type='RandomFlip'),\n                    dict(\n                        type='Normalize',\n                        mean=[123.675, 116.28, 103.53],\n                        std=[58.395, 57.12, 57.375],\n                        to_rgb=True),\n                    dict(type='Pad', size_divisor=32),\n                    dict(type='ImageToTensor', keys=['img']),\n                    dict(type='Collect', keys=['img'])\n                ])\n        ],\n        classes=('smartphone', )),\n    test=dict(\n        type='CocoDataset',\n        ann_file=\n        '/home/aiteam/tykim/scratch/lightweight/mmcv_phone/hold_smartphone/annos/val.json',\n        img_prefix=\n        '/home/aiteam/tykim/scratch/lightweight/mmcv_phone/hold_smartphone/imgs',\n        pipeline=[\n            dict(type='LoadImageFromFile', to_float32=True),\n            dict(\n                type='MultiScaleFlipAug',\n                img_scale=(640, 640),\n                flip=False,\n                transforms=[\n                    dict(type='Resize', keep_ratio=False),\n                    dict(type='RandomFlip'),\n                    dict(\n                        type='Normalize',\n                        mean=[123.675, 116.28, 103.53],\n                        std=[58.395, 57.12, 57.375],\n                        to_rgb=True),\n                    dict(type='Pad', size_divisor=32),\n                    dict(type='ImageToTensor', keys=['img']),\n                    dict(type='Collect', keys=['img'])\n                ])\n        ],\n        classes=('smartphone', )))\nevaluation = dict(interval=10, metric='bbox')\nwork_dir = '/home/aiteam/tykim/scratch/lightweight/mmdet_nano/pico_l_640_exp'\ndevice = 'cuda'\nseed = 0\ngpu_ids = range(0, 8)\n", "seed": 318483671, "exp_name": "l_640.py"}
{"mode": "val", "epoch": 10, "iter": 3, "lr": 0.047, "bbox_mAP": 0.377, "bbox_mAP_50": 0.71, "bbox_mAP_75": 0.42, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.28, "bbox_mAP_l": 0.428, "bbox_mAP_copypaste": "0.377 0.710 0.420 -1.000 0.280 0.428"}
{"mode": "val", "epoch": 20, "iter": 3, "lr": 0.06446, "bbox_mAP": 0.633, "bbox_mAP_50": 0.933, "bbox_mAP_75": 0.641, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.56, "bbox_mAP_l": 0.669, "bbox_mAP_copypaste": "0.633 0.933 0.641 -1.000 0.560 0.669"}
{"mode": "val", "epoch": 30, "iter": 3, "lr": 0.0812, "bbox_mAP": 0.701, "bbox_mAP_50": 0.974, "bbox_mAP_75": 0.888, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.603, "bbox_mAP_l": 0.737, "bbox_mAP_copypaste": "0.701 0.974 0.888 -1.000 0.603 0.737"}
{"mode": "val", "epoch": 40, "iter": 3, "lr": 0.09694, "bbox_mAP": 0.63, "bbox_mAP_50": 0.938, "bbox_mAP_75": 0.776, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.494, "bbox_mAP_l": 0.698, "bbox_mAP_copypaste": "0.630 0.938 0.776 -1.000 0.494 0.698"}
{"mode": "val", "epoch": 50, "iter": 3, "lr": 0.11143, "bbox_mAP": 0.742, "bbox_mAP_50": 0.993, "bbox_mAP_75": 0.911, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.715, "bbox_mAP_l": 0.754, "bbox_mAP_copypaste": "0.742 0.993 0.911 -1.000 0.715 0.754"}
{"mode": "val", "epoch": 60, "iter": 3, "lr": 0.12443, "bbox_mAP": 0.738, "bbox_mAP_50": 0.959, "bbox_mAP_75": 0.822, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.717, "bbox_mAP_l": 0.773, "bbox_mAP_copypaste": "0.738 0.959 0.822 -1.000 0.717 0.773"}
{"mode": "val", "epoch": 70, "iter": 3, "lr": 0.13572, "bbox_mAP": 0.693, "bbox_mAP_50": 0.934, "bbox_mAP_75": 0.866, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.596, "bbox_mAP_l": 0.754, "bbox_mAP_copypaste": "0.693 0.934 0.866 -1.000 0.596 0.754"}
{"mode": "val", "epoch": 80, "iter": 3, "lr": 0.14513, "bbox_mAP": 0.584, "bbox_mAP_50": 0.874, "bbox_mAP_75": 0.735, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.638, "bbox_mAP_l": 0.6, "bbox_mAP_copypaste": "0.584 0.874 0.735 -1.000 0.638 0.600"}
{"mode": "val", "epoch": 90, "iter": 3, "lr": 0.15252, "bbox_mAP": 0.708, "bbox_mAP_50": 0.949, "bbox_mAP_75": 0.839, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.655, "bbox_mAP_l": 0.724, "bbox_mAP_copypaste": "0.708 0.949 0.839 -1.000 0.655 0.724"}
{"mode": "val", "epoch": 100, "iter": 3, "lr": 0.15777, "bbox_mAP": 0.684, "bbox_mAP_50": 0.923, "bbox_mAP_75": 0.835, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.736, "bbox_mAP_l": 0.697, "bbox_mAP_copypaste": "0.684 0.923 0.835 -1.000 0.736 0.697"}
{"mode": "val", "epoch": 110, "iter": 3, "lr": 0.16082, "bbox_mAP": 0.766, "bbox_mAP_50": 0.931, "bbox_mAP_75": 0.931, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.785, "bbox_mAP_l": 0.77, "bbox_mAP_copypaste": "0.766 0.931 0.931 -1.000 0.785 0.770"}
{"mode": "val", "epoch": 120, "iter": 3, "lr": 0.16164, "bbox_mAP": 0.564, "bbox_mAP_50": 0.922, "bbox_mAP_75": 0.623, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.595, "bbox_mAP_l": 0.569, "bbox_mAP_copypaste": "0.564 0.922 0.623 -1.000 0.595 0.569"}
{"mode": "val", "epoch": 130, "iter": 3, "lr": 0.16025, "bbox_mAP": 0.727, "bbox_mAP_50": 0.965, "bbox_mAP_75": 0.965, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.721, "bbox_mAP_l": 0.74, "bbox_mAP_copypaste": "0.727 0.965 0.965 -1.000 0.721 0.740"}
{"mode": "val", "epoch": 140, "iter": 3, "lr": 0.1567, "bbox_mAP": 0.732, "bbox_mAP_50": 0.978, "bbox_mAP_75": 0.892, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.717, "bbox_mAP_l": 0.735, "bbox_mAP_copypaste": "0.732 0.978 0.892 -1.000 0.717 0.735"}
{"mode": "val", "epoch": 150, "iter": 3, "lr": 0.15112, "bbox_mAP": 0.702, "bbox_mAP_50": 0.972, "bbox_mAP_75": 0.889, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.77, "bbox_mAP_l": 0.698, "bbox_mAP_copypaste": "0.702 0.972 0.889 -1.000 0.770 0.698"}
{"mode": "val", "epoch": 160, "iter": 3, "lr": 0.13588, "bbox_mAP": 0.631, "bbox_mAP_50": 0.948, "bbox_mAP_75": 0.778, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.621, "bbox_mAP_l": 0.665, "bbox_mAP_copypaste": "0.631 0.948 0.778 -1.000 0.621 0.665"}
{"mode": "val", "epoch": 170, "iter": 3, "lr": 0.12035, "bbox_mAP": 0.743, "bbox_mAP_50": 0.97, "bbox_mAP_75": 0.92, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.798, "bbox_mAP_l": 0.744, "bbox_mAP_copypaste": "0.743 0.970 0.920 -1.000 0.798 0.744"}
{"mode": "val", "epoch": 180, "iter": 3, "lr": 0.10514, "bbox_mAP": 0.752, "bbox_mAP_50": 0.942, "bbox_mAP_75": 0.896, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.706, "bbox_mAP_l": 0.77, "bbox_mAP_copypaste": "0.752 0.942 0.896 -1.000 0.706 0.770"}
{"mode": "val", "epoch": 190, "iter": 3, "lr": 0.09043, "bbox_mAP": 0.758, "bbox_mAP_50": 0.976, "bbox_mAP_75": 0.922, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.778, "bbox_mAP_l": 0.759, "bbox_mAP_copypaste": "0.758 0.976 0.922 -1.000 0.778 0.759"}
{"mode": "val", "epoch": 200, "iter": 3, "lr": 0.07636, "bbox_mAP": 0.728, "bbox_mAP_50": 0.968, "bbox_mAP_75": 0.838, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.8, "bbox_mAP_l": 0.716, "bbox_mAP_copypaste": "0.728 0.968 0.838 -1.000 0.800 0.716"}
{"mode": "val", "epoch": 210, "iter": 3, "lr": 0.06311, "bbox_mAP": 0.736, "bbox_mAP_50": 0.963, "bbox_mAP_75": 0.824, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.823, "bbox_mAP_l": 0.731, "bbox_mAP_copypaste": "0.736 0.963 0.824 -1.000 0.823 0.731"}
{"mode": "val", "epoch": 220, "iter": 3, "lr": 0.0508, "bbox_mAP": 0.697, "bbox_mAP_50": 0.917, "bbox_mAP_75": 0.812, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.792, "bbox_mAP_l": 0.695, "bbox_mAP_copypaste": "0.697 0.917 0.812 -1.000 0.792 0.695"}
{"mode": "val", "epoch": 230, "iter": 3, "lr": 0.03959, "bbox_mAP": 0.702, "bbox_mAP_50": 0.924, "bbox_mAP_75": 0.799, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.765, "bbox_mAP_l": 0.696, "bbox_mAP_copypaste": "0.702 0.924 0.799 -1.000 0.765 0.696"}
{"mode": "val", "epoch": 240, "iter": 3, "lr": 0.02958, "bbox_mAP": 0.716, "bbox_mAP_50": 0.998, "bbox_mAP_75": 0.85, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.812, "bbox_mAP_l": 0.706, "bbox_mAP_copypaste": "0.716 0.998 0.850 -1.000 0.812 0.706"}
{"mode": "val", "epoch": 250, "iter": 3, "lr": 0.02089, "bbox_mAP": 0.76, "bbox_mAP_50": 0.985, "bbox_mAP_75": 0.862, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.842, "bbox_mAP_l": 0.747, "bbox_mAP_copypaste": "0.760 0.985 0.862 -1.000 0.842 0.747"}
{"mode": "val", "epoch": 260, "iter": 3, "lr": 0.01361, "bbox_mAP": 0.729, "bbox_mAP_50": 0.97, "bbox_mAP_75": 0.818, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.773, "bbox_mAP_l": 0.728, "bbox_mAP_copypaste": "0.729 0.970 0.818 -1.000 0.773 0.728"}
{"mode": "val", "epoch": 270, "iter": 3, "lr": 0.00783, "bbox_mAP": 0.758, "bbox_mAP_50": 0.971, "bbox_mAP_75": 0.855, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.82, "bbox_mAP_l": 0.756, "bbox_mAP_copypaste": "0.758 0.971 0.855 -1.000 0.820 0.756"}
{"mode": "val", "epoch": 280, "iter": 3, "lr": 0.00361, "bbox_mAP": 0.696, "bbox_mAP_50": 0.953, "bbox_mAP_75": 0.847, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.75, "bbox_mAP_l": 0.695, "bbox_mAP_copypaste": "0.696 0.953 0.847 -1.000 0.750 0.695"}
{"mode": "val", "epoch": 290, "iter": 3, "lr": 0.00099, "bbox_mAP": 0.736, "bbox_mAP_50": 0.966, "bbox_mAP_75": 0.81, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.768, "bbox_mAP_l": 0.736, "bbox_mAP_copypaste": "0.736 0.966 0.810 -1.000 0.768 0.736"}
{"mode": "val", "epoch": 300, "iter": 3, "lr": 1e-05, "bbox_mAP": 0.738, "bbox_mAP_50": 0.966, "bbox_mAP_75": 0.814, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.768, "bbox_mAP_l": 0.74, "bbox_mAP_copypaste": "0.738 0.966 0.814 -1.000 0.768 0.740"}
