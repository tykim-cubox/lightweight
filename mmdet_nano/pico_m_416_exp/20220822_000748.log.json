{"env_info": "sys.platform: linux\nPython: 3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA GeForce RTX 3090\nCUDA_HOME: None\nGCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\nPyTorch: 1.8.1+cu111\nPyTorch compiling details: PyTorch built with:\n  - GCC 7.3\n  - C++ Version: 201402\n  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n  - Intel(R) MKL-DNN v1.7.0 (Git Hash 7aed236906b1f7a05c0917e5257a1af05e9ff683)\n  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n  - NNPACK is enabled\n  - CPU capability usage: AVX2\n  - CUDA Runtime 11.1\n  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86\n  - CuDNN 8.0.5\n  - Magma 2.5.2\n  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.8.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, \n\nTorchVision: 0.9.1+cu111\nOpenCV: 4.6.0\nMMCV: 1.4.0\nMMCV Compiler: GCC 7.3\nMMCV CUDA Compiler: 11.1\nMMDetection: 2.19.0+276ecbf", "config": "optimizer = dict(type='SGD', lr=0.4, momentum=0.9, weight_decay=4e-05)\noptimizer_config = dict(grad_clip=None)\nlr_config = dict(\n    policy='CosineAnnealing',\n    min_lr=0,\n    warmup='linear',\n    warmup_iters=300,\n    warmup_ratio=0.1)\nrunner = dict(type='EpochBasedRunner', max_epochs=300)\ncheckpoint_config = dict(interval=10)\nlog_config = dict(\n    interval=20,\n    hooks=[dict(type='TextLoggerHook'),\n           dict(type='TensorboardLoggerHook')])\ncustom_hooks = [\n    dict(type='NumClassCheckHook'),\n    dict(type='CycleEMAHook', cycle_epoch=40, resume_from=None, priority=49)\n]\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nload_from = '/home/aiteam/tykim/scratch/lightweight/mmdet_nano/checkpoints/picodet_m_416_34.2.pth'\nresume_from = None\nworkflow = [('train', 1)]\nmodel = dict(\n    type='PicoDet',\n    backbone=dict(\n        type='ESNet',\n        model_size='m',\n        out_indices=[2, 9, 12],\n        frozen_stages=-1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        norm_eval=False,\n        act_cfg=dict(type='HSwish'),\n        se_cfg=dict(\n            conv_cfg=None,\n            ratio=4,\n            act_cfg=({\n                'type': 'ReLU'\n            }, {\n                'type': 'HSigmoid',\n                'bias': 3.0,\n                'divisor': 6.0,\n                'max_value': 6.0\n            })),\n        init_cfg=dict(\n            type='Pretrained',\n            checkpoint=\n            '/home/aiteam/tykim/scratch/lightweight/mmdet_nano/checkpoints/ESNet_x1_0_pretrained_mmdet_format.pth'\n        )),\n    neck=dict(\n        type='CSPPAN',\n        in_channels=[128, 256, 512],\n        act_cfg=dict(type='HSwish'),\n        norm_cfg=dict(type='BN', requires_grad=True),\n        out_channels=128,\n        num_features=4,\n        expansion=1,\n        num_csp_blocks=1),\n    bbox_head=dict(\n        type='PicoDetHead',\n        num_classes=1,\n        in_channels=128,\n        feat_channels=128,\n        stacked_convs=4,\n        kernel_size=5,\n        share_cls_reg=True,\n        use_depthwise=True,\n        reg_max=7,\n        act_cfg=dict(type='HSwish'),\n        strides=[8, 16, 32, 64],\n        norm_cfg=dict(type='BN', requires_grad=True),\n        use_vfl=True,\n        loss_cls=dict(\n            type='VarifocalLoss',\n            use_sigmoid=True,\n            alpha=0.75,\n            gamma=2.0,\n            iou_weighted=True,\n            loss_weight=1.0),\n        loss_dfl=dict(type='DistributionFocalLoss', loss_weight=0.25),\n        loss_bbox=dict(type='GIoULoss', loss_weight=2.0)),\n    train_cfg=dict(\n        assigner=dict(\n            type='SimOTAAssigner', num_classes=1, use_vfl=True, iou_weight=6),\n        allowed_border=-1,\n        pos_weight=-1,\n        debug=False),\n    test_cfg=dict(\n        nms_pre=1000,\n        min_bbox_size=0,\n        score_thr=0.025,\n        nms=dict(type='nms', iou_threshold=0.6),\n        max_per_img=100))\ndata_root = '/home/aiteam/tykim/scratch/lightweight/mmcv_phone/hold_smartphone'\ndataset_type = 'COCODataset'\nimg_scales = [(352, 352), (384, 384), (416, 416), (448, 448), (480, 480)]\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile', to_float32=True),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(type='MinIoURandomCrop'),\n    dict(\n        type='Resize',\n        img_scale=[(352, 352), (384, 384), (416, 416), (448, 448), (480, 480)],\n        multiscale_mode='value',\n        keep_ratio=False),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(\n        type='PhotoMetricDistortion',\n        brightness_delta=32,\n        contrast_range=(0.5, 1.5),\n        saturation_range=(0.5, 1.5),\n        hue_delta=18),\n    dict(\n        type='Normalize',\n        mean=[123.675, 116.28, 103.53],\n        std=[58.395, 57.12, 57.375],\n        to_rgb=True),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile', to_float32=True),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(416, 416),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=False),\n            dict(type='RandomFlip'),\n            dict(\n                type='Normalize',\n                mean=[123.675, 116.28, 103.53],\n                std=[58.395, 57.12, 57.375],\n                to_rgb=True),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img'])\n        ])\n]\ndata = dict(\n    samples_per_gpu=64,\n    workers_per_gpu=8,\n    persistent_workers=True,\n    train=dict(\n        type='CocoDataset',\n        ann_file=\n        '/home/aiteam/tykim/scratch/lightweight/mmcv_phone/hold_smartphone/annos/train.json',\n        img_prefix=\n        '/home/aiteam/tykim/scratch/lightweight/mmcv_phone/hold_smartphone/imgs',\n        pipeline=[\n            dict(type='LoadImageFromFile', to_float32=True),\n            dict(type='LoadAnnotations', with_bbox=True),\n            dict(type='MinIoURandomCrop'),\n            dict(\n                type='Resize',\n                img_scale=[(352, 352), (384, 384), (416, 416), (448, 448),\n                           (480, 480)],\n                multiscale_mode='value',\n                keep_ratio=False),\n            dict(type='RandomFlip', flip_ratio=0.5),\n            dict(\n                type='PhotoMetricDistortion',\n                brightness_delta=32,\n                contrast_range=(0.5, 1.5),\n                saturation_range=(0.5, 1.5),\n                hue_delta=18),\n            dict(\n                type='Normalize',\n                mean=[123.675, 116.28, 103.53],\n                std=[58.395, 57.12, 57.375],\n                to_rgb=True),\n            dict(type='Pad', size_divisor=32),\n            dict(type='DefaultFormatBundle'),\n            dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])\n        ],\n        classes=('smartphone', )),\n    val=dict(\n        type='CocoDataset',\n        ann_file=\n        '/home/aiteam/tykim/scratch/lightweight/mmcv_phone/hold_smartphone/annos/val.json',\n        img_prefix=\n        '/home/aiteam/tykim/scratch/lightweight/mmcv_phone/hold_smartphone/imgs',\n        pipeline=[\n            dict(type='LoadImageFromFile', to_float32=True),\n            dict(\n                type='MultiScaleFlipAug',\n                img_scale=(416, 416),\n                flip=False,\n                transforms=[\n                    dict(type='Resize', keep_ratio=False),\n                    dict(type='RandomFlip'),\n                    dict(\n                        type='Normalize',\n                        mean=[123.675, 116.28, 103.53],\n                        std=[58.395, 57.12, 57.375],\n                        to_rgb=True),\n                    dict(type='Pad', size_divisor=32),\n                    dict(type='ImageToTensor', keys=['img']),\n                    dict(type='Collect', keys=['img'])\n                ])\n        ],\n        classes=('smartphone', )),\n    test=dict(\n        type='CocoDataset',\n        ann_file=\n        '/home/aiteam/tykim/scratch/lightweight/mmcv_phone/hold_smartphone/annos/val.json',\n        img_prefix=\n        '/home/aiteam/tykim/scratch/lightweight/mmcv_phone/hold_smartphone/imgs',\n        pipeline=[\n            dict(type='LoadImageFromFile', to_float32=True),\n            dict(\n                type='MultiScaleFlipAug',\n                img_scale=(416, 416),\n                flip=False,\n                transforms=[\n                    dict(type='Resize', keep_ratio=False),\n                    dict(type='RandomFlip'),\n                    dict(\n                        type='Normalize',\n                        mean=[123.675, 116.28, 103.53],\n                        std=[58.395, 57.12, 57.375],\n                        to_rgb=True),\n                    dict(type='Pad', size_divisor=32),\n                    dict(type='ImageToTensor', keys=['img']),\n                    dict(type='Collect', keys=['img'])\n                ])\n        ],\n        classes=('smartphone', )))\nevaluation = dict(interval=10, metric='bbox')\nwork_dir = '/home/aiteam/tykim/scratch/lightweight/mmdet_nano/pico_m_416_exp'\ndevice = 'cuda'\nseed = 0\ngpu_ids = range(0, 4)\n", "seed": 726132133, "exp_name": "m_416.py"}
{"mode": "val", "epoch": 10, "iter": 6, "lr": 0.06266, "bbox_mAP": 0.211, "bbox_mAP_50": 0.486, "bbox_mAP_75": 0.125, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.2, "bbox_mAP_l": 0.238, "bbox_mAP_copypaste": "0.211 0.486 0.125 -1.000 0.200 0.238"}
{"mode": "val", "epoch": 20, "iter": 6, "lr": 0.08594, "bbox_mAP": 0.538, "bbox_mAP_50": 0.857, "bbox_mAP_75": 0.529, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.472, "bbox_mAP_l": 0.572, "bbox_mAP_copypaste": "0.538 0.857 0.529 -1.000 0.472 0.572"}
{"mode": "val", "epoch": 30, "iter": 6, "lr": 0.10826, "bbox_mAP": 0.639, "bbox_mAP_50": 0.948, "bbox_mAP_75": 0.825, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.569, "bbox_mAP_l": 0.669, "bbox_mAP_copypaste": "0.639 0.948 0.825 -1.000 0.569 0.669"}
{"mode": "val", "epoch": 40, "iter": 6, "lr": 0.12926, "bbox_mAP": 0.717, "bbox_mAP_50": 0.989, "bbox_mAP_75": 0.903, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.718, "bbox_mAP_l": 0.723, "bbox_mAP_copypaste": "0.717 0.989 0.903 -1.000 0.718 0.723"}
{"mode": "val", "epoch": 50, "iter": 6, "lr": 0.14857, "bbox_mAP": 0.713, "bbox_mAP_50": 0.955, "bbox_mAP_75": 0.872, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.743, "bbox_mAP_l": 0.715, "bbox_mAP_copypaste": "0.713 0.955 0.872 -1.000 0.743 0.715"}
{"mode": "val", "epoch": 60, "iter": 6, "lr": 0.1659, "bbox_mAP": 0.684, "bbox_mAP_50": 0.981, "bbox_mAP_75": 0.791, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.745, "bbox_mAP_l": 0.671, "bbox_mAP_copypaste": "0.684 0.981 0.791 -1.000 0.745 0.671"}
{"mode": "val", "epoch": 70, "iter": 6, "lr": 0.18096, "bbox_mAP": 0.744, "bbox_mAP_50": 0.985, "bbox_mAP_75": 0.856, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.706, "bbox_mAP_l": 0.754, "bbox_mAP_copypaste": "0.744 0.985 0.856 -1.000 0.706 0.754"}
{"mode": "val", "epoch": 80, "iter": 6, "lr": 0.19351, "bbox_mAP": 0.588, "bbox_mAP_50": 0.898, "bbox_mAP_75": 0.797, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.524, "bbox_mAP_l": 0.629, "bbox_mAP_copypaste": "0.588 0.898 0.797 -1.000 0.524 0.629"}
{"mode": "val", "epoch": 90, "iter": 6, "lr": 0.20336, "bbox_mAP": 0.62, "bbox_mAP_50": 0.861, "bbox_mAP_75": 0.799, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.586, "bbox_mAP_l": 0.647, "bbox_mAP_copypaste": "0.620 0.861 0.799 -1.000 0.586 0.647"}
{"mode": "val", "epoch": 100, "iter": 6, "lr": 0.21036, "bbox_mAP": 0.735, "bbox_mAP_50": 0.965, "bbox_mAP_75": 0.914, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.702, "bbox_mAP_l": 0.748, "bbox_mAP_copypaste": "0.735 0.965 0.914 -1.000 0.702 0.748"}
{"mode": "val", "epoch": 110, "iter": 6, "lr": 0.21442, "bbox_mAP": 0.701, "bbox_mAP_50": 0.963, "bbox_mAP_75": 0.845, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.689, "bbox_mAP_l": 0.712, "bbox_mAP_copypaste": "0.701 0.963 0.845 -1.000 0.689 0.712"}
{"mode": "val", "epoch": 120, "iter": 6, "lr": 0.21552, "bbox_mAP": 0.644, "bbox_mAP_50": 0.87, "bbox_mAP_75": 0.777, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.639, "bbox_mAP_l": 0.644, "bbox_mAP_copypaste": "0.644 0.870 0.777 -1.000 0.639 0.644"}
{"mode": "val", "epoch": 130, "iter": 6, "lr": 0.21366, "bbox_mAP": 0.705, "bbox_mAP_50": 0.943, "bbox_mAP_75": 0.835, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.768, "bbox_mAP_l": 0.697, "bbox_mAP_copypaste": "0.705 0.943 0.835 -1.000 0.768 0.697"}
{"mode": "val", "epoch": 140, "iter": 6, "lr": 0.20894, "bbox_mAP": 0.657, "bbox_mAP_50": 0.879, "bbox_mAP_75": 0.82, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.601, "bbox_mAP_l": 0.683, "bbox_mAP_copypaste": "0.657 0.879 0.820 -1.000 0.601 0.683"}
{"mode": "val", "epoch": 150, "iter": 6, "lr": 0.20149, "bbox_mAP": 0.68, "bbox_mAP_50": 0.914, "bbox_mAP_75": 0.867, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.727, "bbox_mAP_l": 0.674, "bbox_mAP_copypaste": "0.680 0.914 0.867 -1.000 0.727 0.674"}
{"mode": "val", "epoch": 160, "iter": 6, "lr": 0.18118, "bbox_mAP": 0.621, "bbox_mAP_50": 0.89, "bbox_mAP_75": 0.742, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.581, "bbox_mAP_l": 0.664, "bbox_mAP_copypaste": "0.621 0.890 0.742 -1.000 0.581 0.664"}
{"mode": "val", "epoch": 170, "iter": 6, "lr": 0.16047, "bbox_mAP": 0.694, "bbox_mAP_50": 0.883, "bbox_mAP_75": 0.869, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.752, "bbox_mAP_l": 0.69, "bbox_mAP_copypaste": "0.694 0.883 0.869 -1.000 0.752 0.690"}
{"mode": "val", "epoch": 180, "iter": 6, "lr": 0.14019, "bbox_mAP": 0.662, "bbox_mAP_50": 0.904, "bbox_mAP_75": 0.762, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.587, "bbox_mAP_l": 0.698, "bbox_mAP_copypaste": "0.662 0.904 0.762 -1.000 0.587 0.698"}
{"mode": "val", "epoch": 190, "iter": 6, "lr": 0.12057, "bbox_mAP": 0.753, "bbox_mAP_50": 0.989, "bbox_mAP_75": 0.888, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.787, "bbox_mAP_l": 0.763, "bbox_mAP_copypaste": "0.753 0.989 0.888 -1.000 0.787 0.763"}
{"mode": "val", "epoch": 200, "iter": 6, "lr": 0.10182, "bbox_mAP": 0.667, "bbox_mAP_50": 0.924, "bbox_mAP_75": 0.805, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.703, "bbox_mAP_l": 0.683, "bbox_mAP_copypaste": "0.667 0.924 0.805 -1.000 0.703 0.683"}
{"mode": "val", "epoch": 210, "iter": 6, "lr": 0.08414, "bbox_mAP": 0.717, "bbox_mAP_50": 0.944, "bbox_mAP_75": 0.898, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.718, "bbox_mAP_l": 0.733, "bbox_mAP_copypaste": "0.717 0.944 0.898 -1.000 0.718 0.733"}
{"mode": "val", "epoch": 220, "iter": 6, "lr": 0.06774, "bbox_mAP": 0.702, "bbox_mAP_50": 0.907, "bbox_mAP_75": 0.863, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.702, "bbox_mAP_l": 0.711, "bbox_mAP_copypaste": "0.702 0.907 0.863 -1.000 0.702 0.711"}
{"mode": "val", "epoch": 230, "iter": 6, "lr": 0.05278, "bbox_mAP": 0.691, "bbox_mAP_50": 0.917, "bbox_mAP_75": 0.877, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.72, "bbox_mAP_l": 0.692, "bbox_mAP_copypaste": "0.691 0.917 0.877 -1.000 0.720 0.692"}
{"mode": "val", "epoch": 240, "iter": 6, "lr": 0.03944, "bbox_mAP": 0.693, "bbox_mAP_50": 0.933, "bbox_mAP_75": 0.926, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.714, "bbox_mAP_l": 0.698, "bbox_mAP_copypaste": "0.693 0.933 0.926 -1.000 0.714 0.698"}
{"mode": "val", "epoch": 250, "iter": 6, "lr": 0.02785, "bbox_mAP": 0.723, "bbox_mAP_50": 0.954, "bbox_mAP_75": 0.854, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.77, "bbox_mAP_l": 0.72, "bbox_mAP_copypaste": "0.723 0.954 0.854 -1.000 0.770 0.720"}
{"mode": "val", "epoch": 260, "iter": 6, "lr": 0.01815, "bbox_mAP": 0.73, "bbox_mAP_50": 0.96, "bbox_mAP_75": 0.885, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.754, "bbox_mAP_l": 0.729, "bbox_mAP_copypaste": "0.730 0.960 0.885 -1.000 0.754 0.729"}
{"mode": "val", "epoch": 270, "iter": 6, "lr": 0.01045, "bbox_mAP": 0.746, "bbox_mAP_50": 0.958, "bbox_mAP_75": 0.912, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.77, "bbox_mAP_l": 0.751, "bbox_mAP_copypaste": "0.746 0.958 0.912 -1.000 0.770 0.751"}
{"mode": "val", "epoch": 280, "iter": 6, "lr": 0.00482, "bbox_mAP": 0.668, "bbox_mAP_50": 0.915, "bbox_mAP_75": 0.871, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.684, "bbox_mAP_l": 0.683, "bbox_mAP_copypaste": "0.668 0.915 0.871 -1.000 0.684 0.683"}
{"mode": "val", "epoch": 290, "iter": 6, "lr": 0.00133, "bbox_mAP": 0.74, "bbox_mAP_50": 0.958, "bbox_mAP_75": 0.912, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.77, "bbox_mAP_l": 0.744, "bbox_mAP_copypaste": "0.740 0.958 0.912 -1.000 0.770 0.744"}
{"mode": "val", "epoch": 300, "iter": 6, "lr": 1e-05, "bbox_mAP": 0.744, "bbox_mAP_50": 0.967, "bbox_mAP_75": 0.918, "bbox_mAP_s": -1.0, "bbox_mAP_m": 0.77, "bbox_mAP_l": 0.745, "bbox_mAP_copypaste": "0.744 0.967 0.918 -1.000 0.770 0.745"}
