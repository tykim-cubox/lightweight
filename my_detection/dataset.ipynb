{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiteam/miniconda3/envs/loader/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "path  = '/home/aiteam/tykim/scratch/lightweight/my_detection/hold_smartphone/annos/train.json'\n",
    "with open(path) as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno = data['annotations'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[338, 212, 45, 46]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anno['bbox']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[338., 212.,  45.,  46.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.as_tensor([anno['bbox']], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'iscrowd': 0,\n",
       " 'image_id': 1,\n",
       " 'bbox': [338, 212, 45, 46],\n",
       " 'segmentation': [],\n",
       " 'category_id': 0,\n",
       " 'id': 1,\n",
       " 'area': 2070}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2070.)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(anno['area'], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'height': 284, 'width': 505, 'id': 1, 'file_name': 'a144.jpg'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_name = data['images'][0]\n",
    "img_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = torch.as_tensor(anno['bbox'], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([338., 212.,  45.,  46.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((1,), dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root, transforms, train):\n",
    "        self.root = root\n",
    "        self.tfms = transforms\n",
    "        # self.imgs = list(sorted(os.listdir(os.path.join(self.root, 'imgs'))))\n",
    "        if train is True:\n",
    "            anno_filename = 'train.json'\n",
    "        else:\n",
    "            anno_filename = 'val.json'\n",
    "                \n",
    "        anno_path = os.path.join(self.root, 'annos', anno_filename)\n",
    "        with open(anno_path) as f:\n",
    "            self.annos = json.load(f)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.annos['annotations'])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.annos['images'][idx]['file_name']\n",
    "        img_path = os.path.join(self.root, 'imgs', img_name)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        anno = self.annos['annotations'][idx]\n",
    "        boxes = torch.as_tensor([anno['bbox']], dtype=torch.float32)\n",
    "        \n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = torch.ones((1,), dtype=torch.int64)\n",
    "        target[\"image_id\"] = torch.tensor([self.annos['images'][idx]['id']])\n",
    "        target[\"area\"] = torch.tensor(anno['area'], dtype=torch.float32)\n",
    "        target[\"iscrowd\"] = anno['iscrowd']\n",
    "\n",
    "        if self.tfms is not None:\n",
    "            img, target = self.tfms(img, target)\n",
    "            \n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/aiteam/tykim/scratch/lightweight/my_detection/hold_smartphone'\n",
    "dataset = CustomDataset(path, None, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=RGB size=1200x801>,\n",
       " {'boxes': tensor([402., 273., 108., 171.]),\n",
       "  'labels': tensor([1]),\n",
       "  'image_id': tensor([3]),\n",
       "  'area': tensor(18468.),\n",
       "  'iscrowd': 0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('loader')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a254999b829cf7a75923305dbce36972a67b91fdc16edd342b076b25e04d6382"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
