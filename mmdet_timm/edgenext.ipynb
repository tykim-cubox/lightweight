{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['edgenext_small', 'edgenext_small_rw', 'edgenext_x_small', 'edgenext_xx_small']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiteam/miniconda3/envs/mm/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "from pprint import pprint\n",
    "model_names = timm.list_models('edge*', pretrained=True)\n",
    "pprint(model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "m = timm.create_model('edgenext_small', pretrained=True)\n",
    "# m.eval()\n",
    "# /home/aiteam/.cache/torch/hub/checkpoints/edgenext_small_usi.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'num_chs': 48, 'reduction': 4, 'module': 'stages.0'},\n",
       " {'num_chs': 96, 'reduction': 8, 'module': 'stages.1'},\n",
       " {'num_chs': 160, 'reduction': 16, 'module': 'stages.2'},\n",
       " {'num_chs': 304, 'reduction': 32, 'module': 'stages.3'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.feature_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture captured\n",
    "\n",
    "import mmcv\n",
    "config = '/home/aiteam/tykim/scratch/lightweight/mmdet_timm/edgenext_exps/faster_rcnn_edgenext_phone.py'\n",
    "cfg = mmcv.Config.fromfile(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.load_from = '/home/aiteam/.cache/torch/hub/checkpoints/edgenext_small_usi.pth'\n",
    "cfg.gpu_ids = range(1)\n",
    "cfg.device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "checkpoint_config = dict(interval=10)\n",
      "log_config = dict(\n",
      "    interval=10,\n",
      "    hooks=[dict(type='TextLoggerHook'),\n",
      "           dict(type='TensorboardLoggerHook')])\n",
      "custom_hooks = [dict(type='NumClassCheckHook')]\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = '/home/aiteam/.cache/torch/hub/checkpoints/edgenext_small_usi.pth'\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "opencv_num_threads = 0\n",
      "mp_start_method = 'fork'\n",
      "auto_scale_lr = dict(enable=False, base_batch_size=16)\n",
      "dataset_type = 'COCODataset'\n",
      "data_root = '/home/aiteam/tykim/scratch/lightweight/mmcv_phone/hold_smartphone'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[103.53, 116.28, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)\n",
      "train_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(type='LoadAnnotations', with_bbox=True),\n",
      "    dict(\n",
      "        type='Resize',\n",
      "        img_scale=[(1333, 640), (1333, 800)],\n",
      "        multiscale_mode='range',\n",
      "        keep_ratio=True),\n",
      "    dict(type='RandomFlip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[103.53, 116.28, 123.675],\n",
      "        std=[1.0, 1.0, 1.0],\n",
      "        to_rgb=False),\n",
      "    dict(type='Pad', size_divisor=32),\n",
      "    dict(type='DefaultFormatBundle'),\n",
      "    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(\n",
      "        type='MultiScaleFlipAug',\n",
      "        img_scale=(1333, 800),\n",
      "        flip=False,\n",
      "        transforms=[\n",
      "            dict(type='Resize', keep_ratio=True),\n",
      "            dict(type='RandomFlip'),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[103.53, 116.28, 123.675],\n",
      "                std=[1.0, 1.0, 1.0],\n",
      "                to_rgb=False),\n",
      "            dict(type='Pad', size_divisor=32),\n",
      "            dict(type='ImageToTensor', keys=['img']),\n",
      "            dict(type='Collect', keys=['img'])\n",
      "        ])\n",
      "]\n",
      "data = dict(\n",
      "    samples_per_gpu=8,\n",
      "    workers_per_gpu=2,\n",
      "    train=dict(\n",
      "        type='RepeatDataset',\n",
      "        times=3,\n",
      "        dataset=dict(\n",
      "            type='CocoDataset',\n",
      "            ann_file=\n",
      "            '/home/aiteam/tykim/scratch/lightweight/mmcv_phone/hold_smartphone/annos/train.json',\n",
      "            img_prefix=\n",
      "            '/home/aiteam/tykim/scratch/lightweight/mmcv_phone/hold_smartphone/imgs',\n",
      "            pipeline=[\n",
      "                dict(type='LoadImageFromFile'),\n",
      "                dict(type='LoadAnnotations', with_bbox=True),\n",
      "                dict(\n",
      "                    type='Resize',\n",
      "                    img_scale=[(1333, 640), (1333, 800)],\n",
      "                    multiscale_mode='range',\n",
      "                    keep_ratio=True),\n",
      "                dict(type='RandomFlip', flip_ratio=0.5),\n",
      "                dict(\n",
      "                    type='Normalize',\n",
      "                    mean=[103.53, 116.28, 123.675],\n",
      "                    std=[1.0, 1.0, 1.0],\n",
      "                    to_rgb=False),\n",
      "                dict(type='Pad', size_divisor=32),\n",
      "                dict(type='DefaultFormatBundle'),\n",
      "                dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])\n",
      "            ],\n",
      "            data_root=\n",
      "            '/home/aiteam/tykim/scratch/lightweight/mmcv_phone/hold_smartphone',\n",
      "            classes=('smartphone', ))),\n",
      "    val=dict(\n",
      "        type='CocoDataset',\n",
      "        ann_file=\n",
      "        '/home/aiteam/tykim/scratch/lightweight/mmcv_phone/hold_smartphone/annos/val.json',\n",
      "        img_prefix=\n",
      "        '/home/aiteam/tykim/scratch/lightweight/mmcv_phone/hold_smartphone/imgs',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(1333, 800),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[103.53, 116.28, 123.675],\n",
      "                        std=[1.0, 1.0, 1.0],\n",
      "                        to_rgb=False),\n",
      "                    dict(type='Pad', size_divisor=32),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='Collect', keys=['img'])\n",
      "                ])\n",
      "        ],\n",
      "        classes=('smartphone', )),\n",
      "    test=dict(\n",
      "        type='CocoDataset',\n",
      "        ann_file=\n",
      "        '/home/aiteam/tykim/scratch/lightweight/mmcv_phone/hold_smartphone/annos/val.json',\n",
      "        img_prefix=\n",
      "        '/home/aiteam/tykim/scratch/lightweight/mmcv_phone/hold_smartphone/imgs',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(1333, 800),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[103.53, 116.28, 123.675],\n",
      "                        std=[1.0, 1.0, 1.0],\n",
      "                        to_rgb=False),\n",
      "                    dict(type='Pad', size_divisor=32),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='Collect', keys=['img'])\n",
      "                ])\n",
      "        ],\n",
      "        classes=('smartphone', )))\n",
      "evaluation = dict(interval=10, metric='bbox')\n",
      "optimizer = dict(type='SGD', lr=0.16, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=None)\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    warmup=None,\n",
      "    warmup_iters=500,\n",
      "    warmup_ratio=0.001,\n",
      "    step=[9, 11])\n",
      "runner = dict(type='EpochBasedRunner', max_epochs=300)\n",
      "model = dict(\n",
      "    type='FasterRCNN',\n",
      "    backbone=dict(\n",
      "        type='mmcls.TIMMBackbone',\n",
      "        model_name='edgenext_small',\n",
      "        features_only=True,\n",
      "        pretrained=True,\n",
      "        out_indices=(0, 1, 2, 3)),\n",
      "    neck=dict(\n",
      "        type='FPN',\n",
      "        in_channels=[48, 96, 160, 304],\n",
      "        out_channels=256,\n",
      "        num_outs=5),\n",
      "    rpn_head=dict(\n",
      "        type='RPNHead',\n",
      "        in_channels=256,\n",
      "        feat_channels=256,\n",
      "        anchor_generator=dict(\n",
      "            type='AnchorGenerator',\n",
      "            scales=[8],\n",
      "            ratios=[0.5, 1.0, 2.0],\n",
      "            strides=[4, 8, 16, 32, 64]),\n",
      "        bbox_coder=dict(\n",
      "            type='DeltaXYWHBBoxCoder',\n",
      "            target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "            target_stds=[1.0, 1.0, 1.0, 1.0]),\n",
      "        loss_cls=dict(\n",
      "            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n",
      "        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),\n",
      "    roi_head=dict(\n",
      "        type='StandardRoIHead',\n",
      "        bbox_roi_extractor=dict(\n",
      "            type='SingleRoIExtractor',\n",
      "            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),\n",
      "            out_channels=256,\n",
      "            featmap_strides=[4, 8, 16, 32]),\n",
      "        bbox_head=dict(\n",
      "            type='Shared2FCBBoxHead',\n",
      "            in_channels=256,\n",
      "            fc_out_channels=1024,\n",
      "            roi_feat_size=7,\n",
      "            num_classes=1,\n",
      "            bbox_coder=dict(\n",
      "                type='DeltaXYWHBBoxCoder',\n",
      "                target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "                target_stds=[0.1, 0.1, 0.2, 0.2]),\n",
      "            reg_class_agnostic=False,\n",
      "            loss_cls=dict(\n",
      "                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n",
      "            loss_bbox=dict(type='L1Loss', loss_weight=1.0))),\n",
      "    train_cfg=dict(\n",
      "        rpn=dict(\n",
      "            assigner=dict(\n",
      "                type='MaxIoUAssigner',\n",
      "                pos_iou_thr=0.7,\n",
      "                neg_iou_thr=0.3,\n",
      "                min_pos_iou=0.3,\n",
      "                match_low_quality=True,\n",
      "                ignore_iof_thr=-1),\n",
      "            sampler=dict(\n",
      "                type='RandomSampler',\n",
      "                num=256,\n",
      "                pos_fraction=0.5,\n",
      "                neg_pos_ub=-1,\n",
      "                add_gt_as_proposals=False),\n",
      "            allowed_border=-1,\n",
      "            pos_weight=-1,\n",
      "            debug=False),\n",
      "        rpn_proposal=dict(\n",
      "            nms_pre=2000,\n",
      "            max_per_img=1000,\n",
      "            nms=dict(type='nms', iou_threshold=0.7),\n",
      "            min_bbox_size=0),\n",
      "        rcnn=dict(\n",
      "            assigner=dict(\n",
      "                type='MaxIoUAssigner',\n",
      "                pos_iou_thr=0.5,\n",
      "                neg_iou_thr=0.5,\n",
      "                min_pos_iou=0.5,\n",
      "                match_low_quality=False,\n",
      "                ignore_iof_thr=-1),\n",
      "            sampler=dict(\n",
      "                type='RandomSampler',\n",
      "                num=512,\n",
      "                pos_fraction=0.25,\n",
      "                neg_pos_ub=-1,\n",
      "                add_gt_as_proposals=True),\n",
      "            pos_weight=-1,\n",
      "            debug=False)),\n",
      "    test_cfg=dict(\n",
      "        rpn=dict(\n",
      "            nms_pre=1000,\n",
      "            max_per_img=1000,\n",
      "            nms=dict(type='nms', iou_threshold=0.7),\n",
      "            min_bbox_size=0),\n",
      "        rcnn=dict(\n",
      "            score_thr=0.05,\n",
      "            nms=dict(type='nms', iou_threshold=0.5),\n",
      "            max_per_img=100)))\n",
      "work_dir = '/home/aiteam/tykim/scratch/lightweight/mmdet_timm/edgenext_exps'\n",
      "seed = 0\n",
      "custom_imports = dict(imports=['mmcls.models'], allow_failed_imports=False)\n",
      "auto_resume = False\n",
      "gpu_ids = range(0, 1)\n",
      "device = 'cuda'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋과 detector를 init한다.\n",
    "from mmdet.datasets import build_dataset\n",
    "from mmdet.models import build_detector\n",
    "from mmdet.apis import train_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "datasets = [build_dataset(cfg.data.train.dataset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-23 07:10:14,203 - mmcls - INFO - backbone out_indices: (0, 1, 2, 3)\n",
      "2022-08-23 07:10:14,204 - mmcls - INFO - backbone out_channels: [48, 96, 160, 304]\n",
      "2022-08-23 07:10:14,205 - mmcls - INFO - backbone out_strides: [4, 8, 16, 32]\n"
     ]
    }
   ],
   "source": [
    "model = build_detector(cfg.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.CLASSES = datasets[0].CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-23 07:10:19,200 - mmdet - INFO - Automatic scaling of learning rate (LR) has been disabled.\n",
      "2022-08-23 07:10:19,210 - mmdet - INFO - load checkpoint from local path: /home/aiteam/.cache/torch/hub/checkpoints/edgenext_small_usi.pth\n",
      "2022-08-23 07:10:19,232 - mmdet - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "unexpected key in source state_dict: downsample_layers.0.0.weight, downsample_layers.0.0.bias, downsample_layers.0.1.weight, downsample_layers.0.1.bias, downsample_layers.1.0.weight, downsample_layers.1.0.bias, downsample_layers.1.1.weight, downsample_layers.1.1.bias, downsample_layers.2.0.weight, downsample_layers.2.0.bias, downsample_layers.2.1.weight, downsample_layers.2.1.bias, downsample_layers.3.0.weight, downsample_layers.3.0.bias, downsample_layers.3.1.weight, downsample_layers.3.1.bias, stages.0.0.gamma, stages.0.0.dwconv.weight, stages.0.0.dwconv.bias, stages.0.0.norm.weight, stages.0.0.norm.bias, stages.0.0.pwconv1.weight, stages.0.0.pwconv1.bias, stages.0.0.pwconv2.weight, stages.0.0.pwconv2.bias, stages.0.1.gamma, stages.0.1.dwconv.weight, stages.0.1.dwconv.bias, stages.0.1.norm.weight, stages.0.1.norm.bias, stages.0.1.pwconv1.weight, stages.0.1.pwconv1.bias, stages.0.1.pwconv2.weight, stages.0.1.pwconv2.bias, stages.0.2.gamma, stages.0.2.dwconv.weight, stages.0.2.dwconv.bias, stages.0.2.norm.weight, stages.0.2.norm.bias, stages.0.2.pwconv1.weight, stages.0.2.pwconv1.bias, stages.0.2.pwconv2.weight, stages.0.2.pwconv2.bias, stages.1.0.gamma, stages.1.0.dwconv.weight, stages.1.0.dwconv.bias, stages.1.0.norm.weight, stages.1.0.norm.bias, stages.1.0.pwconv1.weight, stages.1.0.pwconv1.bias, stages.1.0.pwconv2.weight, stages.1.0.pwconv2.bias, stages.1.1.gamma, stages.1.1.dwconv.weight, stages.1.1.dwconv.bias, stages.1.1.norm.weight, stages.1.1.norm.bias, stages.1.1.pwconv1.weight, stages.1.1.pwconv1.bias, stages.1.1.pwconv2.weight, stages.1.1.pwconv2.bias, stages.1.2.gamma_xca, stages.1.2.gamma, stages.1.2.convs.0.weight, stages.1.2.convs.0.bias, stages.1.2.pos_embd.token_projection.weight, stages.1.2.pos_embd.token_projection.bias, stages.1.2.norm_xca.weight, stages.1.2.norm_xca.bias, stages.1.2.xca.temperature, stages.1.2.xca.qkv.weight, stages.1.2.xca.qkv.bias, stages.1.2.xca.proj.weight, stages.1.2.xca.proj.bias, stages.1.2.norm.weight, stages.1.2.norm.bias, stages.1.2.pwconv1.weight, stages.1.2.pwconv1.bias, stages.1.2.pwconv2.weight, stages.1.2.pwconv2.bias, stages.2.0.gamma, stages.2.0.dwconv.weight, stages.2.0.dwconv.bias, stages.2.0.norm.weight, stages.2.0.norm.bias, stages.2.0.pwconv1.weight, stages.2.0.pwconv1.bias, stages.2.0.pwconv2.weight, stages.2.0.pwconv2.bias, stages.2.1.gamma, stages.2.1.dwconv.weight, stages.2.1.dwconv.bias, stages.2.1.norm.weight, stages.2.1.norm.bias, stages.2.1.pwconv1.weight, stages.2.1.pwconv1.bias, stages.2.1.pwconv2.weight, stages.2.1.pwconv2.bias, stages.2.2.gamma, stages.2.2.dwconv.weight, stages.2.2.dwconv.bias, stages.2.2.norm.weight, stages.2.2.norm.bias, stages.2.2.pwconv1.weight, stages.2.2.pwconv1.bias, stages.2.2.pwconv2.weight, stages.2.2.pwconv2.bias, stages.2.3.gamma, stages.2.3.dwconv.weight, stages.2.3.dwconv.bias, stages.2.3.norm.weight, stages.2.3.norm.bias, stages.2.3.pwconv1.weight, stages.2.3.pwconv1.bias, stages.2.3.pwconv2.weight, stages.2.3.pwconv2.bias, stages.2.4.gamma, stages.2.4.dwconv.weight, stages.2.4.dwconv.bias, stages.2.4.norm.weight, stages.2.4.norm.bias, stages.2.4.pwconv1.weight, stages.2.4.pwconv1.bias, stages.2.4.pwconv2.weight, stages.2.4.pwconv2.bias, stages.2.5.gamma, stages.2.5.dwconv.weight, stages.2.5.dwconv.bias, stages.2.5.norm.weight, stages.2.5.norm.bias, stages.2.5.pwconv1.weight, stages.2.5.pwconv1.bias, stages.2.5.pwconv2.weight, stages.2.5.pwconv2.bias, stages.2.6.gamma, stages.2.6.dwconv.weight, stages.2.6.dwconv.bias, stages.2.6.norm.weight, stages.2.6.norm.bias, stages.2.6.pwconv1.weight, stages.2.6.pwconv1.bias, stages.2.6.pwconv2.weight, stages.2.6.pwconv2.bias, stages.2.7.gamma, stages.2.7.dwconv.weight, stages.2.7.dwconv.bias, stages.2.7.norm.weight, stages.2.7.norm.bias, stages.2.7.pwconv1.weight, stages.2.7.pwconv1.bias, stages.2.7.pwconv2.weight, stages.2.7.pwconv2.bias, stages.2.8.gamma_xca, stages.2.8.gamma, stages.2.8.convs.0.weight, stages.2.8.convs.0.bias, stages.2.8.convs.1.weight, stages.2.8.convs.1.bias, stages.2.8.norm_xca.weight, stages.2.8.norm_xca.bias, stages.2.8.xca.temperature, stages.2.8.xca.qkv.weight, stages.2.8.xca.qkv.bias, stages.2.8.xca.proj.weight, stages.2.8.xca.proj.bias, stages.2.8.norm.weight, stages.2.8.norm.bias, stages.2.8.pwconv1.weight, stages.2.8.pwconv1.bias, stages.2.8.pwconv2.weight, stages.2.8.pwconv2.bias, stages.3.0.gamma, stages.3.0.dwconv.weight, stages.3.0.dwconv.bias, stages.3.0.norm.weight, stages.3.0.norm.bias, stages.3.0.pwconv1.weight, stages.3.0.pwconv1.bias, stages.3.0.pwconv2.weight, stages.3.0.pwconv2.bias, stages.3.1.gamma, stages.3.1.dwconv.weight, stages.3.1.dwconv.bias, stages.3.1.norm.weight, stages.3.1.norm.bias, stages.3.1.pwconv1.weight, stages.3.1.pwconv1.bias, stages.3.1.pwconv2.weight, stages.3.1.pwconv2.bias, stages.3.2.gamma_xca, stages.3.2.gamma, stages.3.2.convs.0.weight, stages.3.2.convs.0.bias, stages.3.2.convs.1.weight, stages.3.2.convs.1.bias, stages.3.2.convs.2.weight, stages.3.2.convs.2.bias, stages.3.2.norm_xca.weight, stages.3.2.norm_xca.bias, stages.3.2.xca.temperature, stages.3.2.xca.qkv.weight, stages.3.2.xca.qkv.bias, stages.3.2.xca.proj.weight, stages.3.2.xca.proj.bias, stages.3.2.norm.weight, stages.3.2.norm.bias, stages.3.2.pwconv1.weight, stages.3.2.pwconv1.bias, stages.3.2.pwconv2.weight, stages.3.2.pwconv2.bias, norm.weight, norm.bias, head.weight, head.bias\n",
      "\n",
      "missing keys in source state_dict: backbone.timm_model.stem_0.weight, backbone.timm_model.stem_0.bias, backbone.timm_model.stem_1.weight, backbone.timm_model.stem_1.bias, backbone.timm_model.stages_0.blocks.0.gamma, backbone.timm_model.stages_0.blocks.0.conv_dw.weight, backbone.timm_model.stages_0.blocks.0.conv_dw.bias, backbone.timm_model.stages_0.blocks.0.norm.weight, backbone.timm_model.stages_0.blocks.0.norm.bias, backbone.timm_model.stages_0.blocks.0.mlp.fc1.weight, backbone.timm_model.stages_0.blocks.0.mlp.fc1.bias, backbone.timm_model.stages_0.blocks.0.mlp.fc2.weight, backbone.timm_model.stages_0.blocks.0.mlp.fc2.bias, backbone.timm_model.stages_0.blocks.1.gamma, backbone.timm_model.stages_0.blocks.1.conv_dw.weight, backbone.timm_model.stages_0.blocks.1.conv_dw.bias, backbone.timm_model.stages_0.blocks.1.norm.weight, backbone.timm_model.stages_0.blocks.1.norm.bias, backbone.timm_model.stages_0.blocks.1.mlp.fc1.weight, backbone.timm_model.stages_0.blocks.1.mlp.fc1.bias, backbone.timm_model.stages_0.blocks.1.mlp.fc2.weight, backbone.timm_model.stages_0.blocks.1.mlp.fc2.bias, backbone.timm_model.stages_0.blocks.2.gamma, backbone.timm_model.stages_0.blocks.2.conv_dw.weight, backbone.timm_model.stages_0.blocks.2.conv_dw.bias, backbone.timm_model.stages_0.blocks.2.norm.weight, backbone.timm_model.stages_0.blocks.2.norm.bias, backbone.timm_model.stages_0.blocks.2.mlp.fc1.weight, backbone.timm_model.stages_0.blocks.2.mlp.fc1.bias, backbone.timm_model.stages_0.blocks.2.mlp.fc2.weight, backbone.timm_model.stages_0.blocks.2.mlp.fc2.bias, backbone.timm_model.stages_1.downsample.0.weight, backbone.timm_model.stages_1.downsample.0.bias, backbone.timm_model.stages_1.downsample.1.weight, backbone.timm_model.stages_1.downsample.1.bias, backbone.timm_model.stages_1.blocks.0.gamma, backbone.timm_model.stages_1.blocks.0.conv_dw.weight, backbone.timm_model.stages_1.blocks.0.conv_dw.bias, backbone.timm_model.stages_1.blocks.0.norm.weight, backbone.timm_model.stages_1.blocks.0.norm.bias, backbone.timm_model.stages_1.blocks.0.mlp.fc1.weight, backbone.timm_model.stages_1.blocks.0.mlp.fc1.bias, backbone.timm_model.stages_1.blocks.0.mlp.fc2.weight, backbone.timm_model.stages_1.blocks.0.mlp.fc2.bias, backbone.timm_model.stages_1.blocks.1.gamma, backbone.timm_model.stages_1.blocks.1.conv_dw.weight, backbone.timm_model.stages_1.blocks.1.conv_dw.bias, backbone.timm_model.stages_1.blocks.1.norm.weight, backbone.timm_model.stages_1.blocks.1.norm.bias, backbone.timm_model.stages_1.blocks.1.mlp.fc1.weight, backbone.timm_model.stages_1.blocks.1.mlp.fc1.bias, backbone.timm_model.stages_1.blocks.1.mlp.fc2.weight, backbone.timm_model.stages_1.blocks.1.mlp.fc2.bias, backbone.timm_model.stages_1.blocks.2.gamma_xca, backbone.timm_model.stages_1.blocks.2.gamma, backbone.timm_model.stages_1.blocks.2.convs.0.weight, backbone.timm_model.stages_1.blocks.2.convs.0.bias, backbone.timm_model.stages_1.blocks.2.norm_xca.weight, backbone.timm_model.stages_1.blocks.2.norm_xca.bias, backbone.timm_model.stages_1.blocks.2.xca.temperature, backbone.timm_model.stages_1.blocks.2.xca.qkv.weight, backbone.timm_model.stages_1.blocks.2.xca.qkv.bias, backbone.timm_model.stages_1.blocks.2.xca.proj.weight, backbone.timm_model.stages_1.blocks.2.xca.proj.bias, backbone.timm_model.stages_1.blocks.2.norm.weight, backbone.timm_model.stages_1.blocks.2.norm.bias, backbone.timm_model.stages_1.blocks.2.mlp.fc1.weight, backbone.timm_model.stages_1.blocks.2.mlp.fc1.bias, backbone.timm_model.stages_1.blocks.2.mlp.fc2.weight, backbone.timm_model.stages_1.blocks.2.mlp.fc2.bias, backbone.timm_model.stages_2.downsample.0.weight, backbone.timm_model.stages_2.downsample.0.bias, backbone.timm_model.stages_2.downsample.1.weight, backbone.timm_model.stages_2.downsample.1.bias, backbone.timm_model.stages_2.blocks.0.gamma, backbone.timm_model.stages_2.blocks.0.conv_dw.weight, backbone.timm_model.stages_2.blocks.0.conv_dw.bias, backbone.timm_model.stages_2.blocks.0.norm.weight, backbone.timm_model.stages_2.blocks.0.norm.bias, backbone.timm_model.stages_2.blocks.0.mlp.fc1.weight, backbone.timm_model.stages_2.blocks.0.mlp.fc1.bias, backbone.timm_model.stages_2.blocks.0.mlp.fc2.weight, backbone.timm_model.stages_2.blocks.0.mlp.fc2.bias, backbone.timm_model.stages_2.blocks.1.gamma, backbone.timm_model.stages_2.blocks.1.conv_dw.weight, backbone.timm_model.stages_2.blocks.1.conv_dw.bias, backbone.timm_model.stages_2.blocks.1.norm.weight, backbone.timm_model.stages_2.blocks.1.norm.bias, backbone.timm_model.stages_2.blocks.1.mlp.fc1.weight, backbone.timm_model.stages_2.blocks.1.mlp.fc1.bias, backbone.timm_model.stages_2.blocks.1.mlp.fc2.weight, backbone.timm_model.stages_2.blocks.1.mlp.fc2.bias, backbone.timm_model.stages_2.blocks.2.gamma, backbone.timm_model.stages_2.blocks.2.conv_dw.weight, backbone.timm_model.stages_2.blocks.2.conv_dw.bias, backbone.timm_model.stages_2.blocks.2.norm.weight, backbone.timm_model.stages_2.blocks.2.norm.bias, backbone.timm_model.stages_2.blocks.2.mlp.fc1.weight, backbone.timm_model.stages_2.blocks.2.mlp.fc1.bias, backbone.timm_model.stages_2.blocks.2.mlp.fc2.weight, backbone.timm_model.stages_2.blocks.2.mlp.fc2.bias, backbone.timm_model.stages_2.blocks.3.gamma, backbone.timm_model.stages_2.blocks.3.conv_dw.weight, backbone.timm_model.stages_2.blocks.3.conv_dw.bias, backbone.timm_model.stages_2.blocks.3.norm.weight, backbone.timm_model.stages_2.blocks.3.norm.bias, backbone.timm_model.stages_2.blocks.3.mlp.fc1.weight, backbone.timm_model.stages_2.blocks.3.mlp.fc1.bias, backbone.timm_model.stages_2.blocks.3.mlp.fc2.weight, backbone.timm_model.stages_2.blocks.3.mlp.fc2.bias, backbone.timm_model.stages_2.blocks.4.gamma, backbone.timm_model.stages_2.blocks.4.conv_dw.weight, backbone.timm_model.stages_2.blocks.4.conv_dw.bias, backbone.timm_model.stages_2.blocks.4.norm.weight, backbone.timm_model.stages_2.blocks.4.norm.bias, backbone.timm_model.stages_2.blocks.4.mlp.fc1.weight, backbone.timm_model.stages_2.blocks.4.mlp.fc1.bias, backbone.timm_model.stages_2.blocks.4.mlp.fc2.weight, backbone.timm_model.stages_2.blocks.4.mlp.fc2.bias, backbone.timm_model.stages_2.blocks.5.gamma, backbone.timm_model.stages_2.blocks.5.conv_dw.weight, backbone.timm_model.stages_2.blocks.5.conv_dw.bias, backbone.timm_model.stages_2.blocks.5.norm.weight, backbone.timm_model.stages_2.blocks.5.norm.bias, backbone.timm_model.stages_2.blocks.5.mlp.fc1.weight, backbone.timm_model.stages_2.blocks.5.mlp.fc1.bias, backbone.timm_model.stages_2.blocks.5.mlp.fc2.weight, backbone.timm_model.stages_2.blocks.5.mlp.fc2.bias, backbone.timm_model.stages_2.blocks.6.gamma, backbone.timm_model.stages_2.blocks.6.conv_dw.weight, backbone.timm_model.stages_2.blocks.6.conv_dw.bias, backbone.timm_model.stages_2.blocks.6.norm.weight, backbone.timm_model.stages_2.blocks.6.norm.bias, backbone.timm_model.stages_2.blocks.6.mlp.fc1.weight, backbone.timm_model.stages_2.blocks.6.mlp.fc1.bias, backbone.timm_model.stages_2.blocks.6.mlp.fc2.weight, backbone.timm_model.stages_2.blocks.6.mlp.fc2.bias, backbone.timm_model.stages_2.blocks.7.gamma, backbone.timm_model.stages_2.blocks.7.conv_dw.weight, backbone.timm_model.stages_2.blocks.7.conv_dw.bias, backbone.timm_model.stages_2.blocks.7.norm.weight, backbone.timm_model.stages_2.blocks.7.norm.bias, backbone.timm_model.stages_2.blocks.7.mlp.fc1.weight, backbone.timm_model.stages_2.blocks.7.mlp.fc1.bias, backbone.timm_model.stages_2.blocks.7.mlp.fc2.weight, backbone.timm_model.stages_2.blocks.7.mlp.fc2.bias, backbone.timm_model.stages_2.blocks.8.gamma_xca, backbone.timm_model.stages_2.blocks.8.gamma, backbone.timm_model.stages_2.blocks.8.convs.0.weight, backbone.timm_model.stages_2.blocks.8.convs.0.bias, backbone.timm_model.stages_2.blocks.8.convs.1.weight, backbone.timm_model.stages_2.blocks.8.convs.1.bias, backbone.timm_model.stages_2.blocks.8.norm_xca.weight, backbone.timm_model.stages_2.blocks.8.norm_xca.bias, backbone.timm_model.stages_2.blocks.8.xca.temperature, backbone.timm_model.stages_2.blocks.8.xca.qkv.weight, backbone.timm_model.stages_2.blocks.8.xca.qkv.bias, backbone.timm_model.stages_2.blocks.8.xca.proj.weight, backbone.timm_model.stages_2.blocks.8.xca.proj.bias, backbone.timm_model.stages_2.blocks.8.norm.weight, backbone.timm_model.stages_2.blocks.8.norm.bias, backbone.timm_model.stages_2.blocks.8.mlp.fc1.weight, backbone.timm_model.stages_2.blocks.8.mlp.fc1.bias, backbone.timm_model.stages_2.blocks.8.mlp.fc2.weight, backbone.timm_model.stages_2.blocks.8.mlp.fc2.bias, backbone.timm_model.stages_3.downsample.0.weight, backbone.timm_model.stages_3.downsample.0.bias, backbone.timm_model.stages_3.downsample.1.weight, backbone.timm_model.stages_3.downsample.1.bias, backbone.timm_model.stages_3.blocks.0.gamma, backbone.timm_model.stages_3.blocks.0.conv_dw.weight, backbone.timm_model.stages_3.blocks.0.conv_dw.bias, backbone.timm_model.stages_3.blocks.0.norm.weight, backbone.timm_model.stages_3.blocks.0.norm.bias, backbone.timm_model.stages_3.blocks.0.mlp.fc1.weight, backbone.timm_model.stages_3.blocks.0.mlp.fc1.bias, backbone.timm_model.stages_3.blocks.0.mlp.fc2.weight, backbone.timm_model.stages_3.blocks.0.mlp.fc2.bias, backbone.timm_model.stages_3.blocks.1.gamma, backbone.timm_model.stages_3.blocks.1.conv_dw.weight, backbone.timm_model.stages_3.blocks.1.conv_dw.bias, backbone.timm_model.stages_3.blocks.1.norm.weight, backbone.timm_model.stages_3.blocks.1.norm.bias, backbone.timm_model.stages_3.blocks.1.mlp.fc1.weight, backbone.timm_model.stages_3.blocks.1.mlp.fc1.bias, backbone.timm_model.stages_3.blocks.1.mlp.fc2.weight, backbone.timm_model.stages_3.blocks.1.mlp.fc2.bias, backbone.timm_model.stages_3.blocks.2.gamma_xca, backbone.timm_model.stages_3.blocks.2.gamma, backbone.timm_model.stages_3.blocks.2.convs.0.weight, backbone.timm_model.stages_3.blocks.2.convs.0.bias, backbone.timm_model.stages_3.blocks.2.convs.1.weight, backbone.timm_model.stages_3.blocks.2.convs.1.bias, backbone.timm_model.stages_3.blocks.2.convs.2.weight, backbone.timm_model.stages_3.blocks.2.convs.2.bias, backbone.timm_model.stages_3.blocks.2.norm_xca.weight, backbone.timm_model.stages_3.blocks.2.norm_xca.bias, backbone.timm_model.stages_3.blocks.2.xca.temperature, backbone.timm_model.stages_3.blocks.2.xca.qkv.weight, backbone.timm_model.stages_3.blocks.2.xca.qkv.bias, backbone.timm_model.stages_3.blocks.2.xca.proj.weight, backbone.timm_model.stages_3.blocks.2.xca.proj.bias, backbone.timm_model.stages_3.blocks.2.norm.weight, backbone.timm_model.stages_3.blocks.2.norm.bias, backbone.timm_model.stages_3.blocks.2.mlp.fc1.weight, backbone.timm_model.stages_3.blocks.2.mlp.fc1.bias, backbone.timm_model.stages_3.blocks.2.mlp.fc2.weight, backbone.timm_model.stages_3.blocks.2.mlp.fc2.bias, neck.lateral_convs.0.conv.weight, neck.lateral_convs.0.conv.bias, neck.lateral_convs.1.conv.weight, neck.lateral_convs.1.conv.bias, neck.lateral_convs.2.conv.weight, neck.lateral_convs.2.conv.bias, neck.lateral_convs.3.conv.weight, neck.lateral_convs.3.conv.bias, neck.fpn_convs.0.conv.weight, neck.fpn_convs.0.conv.bias, neck.fpn_convs.1.conv.weight, neck.fpn_convs.1.conv.bias, neck.fpn_convs.2.conv.weight, neck.fpn_convs.2.conv.bias, neck.fpn_convs.3.conv.weight, neck.fpn_convs.3.conv.bias, rpn_head.rpn_conv.weight, rpn_head.rpn_conv.bias, rpn_head.rpn_cls.weight, rpn_head.rpn_cls.bias, rpn_head.rpn_reg.weight, rpn_head.rpn_reg.bias, roi_head.bbox_head.fc_cls.weight, roi_head.bbox_head.fc_cls.bias, roi_head.bbox_head.fc_reg.weight, roi_head.bbox_head.fc_reg.bias, roi_head.bbox_head.shared_fcs.0.weight, roi_head.bbox_head.shared_fcs.0.bias, roi_head.bbox_head.shared_fcs.1.weight, roi_head.bbox_head.shared_fcs.1.bias\n",
      "\n",
      "2022-08-23 07:10:19,234 - mmdet - INFO - Start running, host: aiteam@aiteam, work_dir: /home/aiteam/tykim/scratch/lightweight/mmdet_timm/edgenext_exps\n",
      "2022-08-23 07:10:19,235 - mmdet - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(LOW         ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      "(VERY_LOW    ) TensorboardLoggerHook              \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) NumClassCheckHook                  \n",
      "(LOW         ) IterTimerHook                      \n",
      "(LOW         ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      "(VERY_LOW    ) TensorboardLoggerHook              \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(LOW         ) IterTimerHook                      \n",
      "(LOW         ) EvalHook                           \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(LOW         ) IterTimerHook                      \n",
      "(LOW         ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      "(VERY_LOW    ) TensorboardLoggerHook              \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(LOW         ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      "(VERY_LOW    ) TensorboardLoggerHook              \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(NORMAL      ) NumClassCheckHook                  \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      "(VERY_LOW    ) TensorboardLoggerHook              \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      "(VERY_LOW    ) TensorboardLoggerHook              \n",
      " -------------------- \n",
      "after_run:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      "(VERY_LOW    ) TensorboardLoggerHook              \n",
      " -------------------- \n",
      "2022-08-23 07:10:19,236 - mmdet - INFO - workflow: [('train', 1)], max: 300 epochs\n",
      "2022-08-23 07:10:19,237 - mmdet - INFO - Checkpoints will be saved to /home/aiteam/tykim/scratch/lightweight/mmdet_timm/edgenext_exps by HardDiskBackend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/aiteam/tykim/scratch/lightweight/mmdet_timm/edgenext.ipynb 셀 11\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B172.16.100.102/home/aiteam/tykim/scratch/lightweight/mmdet_timm/edgenext.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m train_detector(model, datasets, cfg, distributed\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, validate\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/tykim/scratch/lightweight/mmcv_phone/mmdetection/mmdet/apis/train.py:244\u001b[0m, in \u001b[0;36mtrain_detector\u001b[0;34m(model, dataset, cfg, distributed, validate, timestamp, meta)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[39melif\u001b[39;00m cfg\u001b[39m.\u001b[39mload_from:\n\u001b[1;32m    243\u001b[0m     runner\u001b[39m.\u001b[39mload_checkpoint(cfg\u001b[39m.\u001b[39mload_from)\n\u001b[0;32m--> 244\u001b[0m runner\u001b[39m.\u001b[39;49mrun(data_loaders, cfg\u001b[39m.\u001b[39;49mworkflow)\n",
      "File \u001b[0;32m~/miniconda3/envs/mm/lib/python3.8/site-packages/mmcv/runner/epoch_based_runner.py:136\u001b[0m, in \u001b[0;36mEpochBasedRunner.run\u001b[0;34m(self, data_loaders, workflow, max_epochs, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_epochs:\n\u001b[1;32m    135\u001b[0m                 \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m             epoch_runner(data_loaders[i], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    138\u001b[0m time\u001b[39m.\u001b[39msleep(\u001b[39m1\u001b[39m)  \u001b[39m# wait for some hooks like loggers to finish\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_hook(\u001b[39m'\u001b[39m\u001b[39mafter_run\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/mm/lib/python3.8/site-packages/mmcv/runner/epoch_based_runner.py:53\u001b[0m, in \u001b[0;36mEpochBasedRunner.train\u001b[0;34m(self, data_loader, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_iter \u001b[39m=\u001b[39m i\n\u001b[1;32m     52\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_hook(\u001b[39m'\u001b[39m\u001b[39mbefore_train_iter\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 53\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_iter(data_batch, train_mode\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     54\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_hook(\u001b[39m'\u001b[39m\u001b[39mafter_train_iter\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_batch\n",
      "File \u001b[0;32m~/miniconda3/envs/mm/lib/python3.8/site-packages/mmcv/runner/epoch_based_runner.py:31\u001b[0m, in \u001b[0;36mEpochBasedRunner.run_iter\u001b[0;34m(self, data_batch, train_mode, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_processor(\n\u001b[1;32m     29\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, data_batch, train_mode\u001b[39m=\u001b[39mtrain_mode, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     30\u001b[0m \u001b[39melif\u001b[39;00m train_mode:\n\u001b[0;32m---> 31\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mtrain_step(data_batch, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer,\n\u001b[1;32m     32\u001b[0m                                     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     33\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mval_step(data_batch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/mm/lib/python3.8/site-packages/mmcv/parallel/data_parallel.py:77\u001b[0m, in \u001b[0;36mMMDataParallel.train_step\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m     72\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mmodule must have its parameters and buffers \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     73\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mon device \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msrc_device_obj\u001b[39m}\u001b[39;00m\u001b[39m (device_ids[0]) but \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     74\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfound one of them on device: \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m.\u001b[39mdevice\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     76\u001b[0m inputs, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscatter(inputs, kwargs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids)\n\u001b[0;32m---> 77\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodule\u001b[39m.\u001b[39;49mtrain_step(\u001b[39m*\u001b[39;49minputs[\u001b[39m0\u001b[39;49m], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs[\u001b[39m0\u001b[39;49m])\n",
      "File \u001b[0;32m~/tykim/scratch/lightweight/mmcv_phone/mmdetection/mmdet/models/detectors/base.py:248\u001b[0m, in \u001b[0;36mBaseDetector.train_step\u001b[0;34m(self, data, optimizer)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_step\u001b[39m(\u001b[39mself\u001b[39m, data, optimizer):\n\u001b[1;32m    222\u001b[0m     \u001b[39m\"\"\"The iteration step during training.\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \n\u001b[1;32m    224\u001b[0m \u001b[39m    This method defines an iteration step during training, except for the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[39m              averaging the logs.\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m     losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdata)\n\u001b[1;32m    249\u001b[0m     loss, log_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parse_losses(losses)\n\u001b[1;32m    251\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[1;32m    252\u001b[0m         loss\u001b[39m=\u001b[39mloss, log_vars\u001b[39m=\u001b[39mlog_vars, num_samples\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(data[\u001b[39m'\u001b[39m\u001b[39mimg_metas\u001b[39m\u001b[39m'\u001b[39m]))\n",
      "File \u001b[0;32m~/miniconda3/envs/mm/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    890\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/miniconda3/envs/mm/lib/python3.8/site-packages/mmcv/runner/fp16_utils.py:116\u001b[0m, in \u001b[0;36mauto_fp16.<locals>.auto_fp16_wrapper.<locals>.new_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m@auto_fp16 can only be used to decorate the \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    114\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmethod of those classes \u001b[39m\u001b[39m{\u001b[39;00msupported_types\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mhasattr\u001b[39m(args[\u001b[39m0\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mfp16_enabled\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m args[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mfp16_enabled):\n\u001b[0;32m--> 116\u001b[0m     \u001b[39mreturn\u001b[39;00m old_func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    118\u001b[0m \u001b[39m# get the arg spec of the decorated method\u001b[39;00m\n\u001b[1;32m    119\u001b[0m args_info \u001b[39m=\u001b[39m getfullargspec(old_func)\n",
      "File \u001b[0;32m~/tykim/scratch/lightweight/mmcv_phone/mmdetection/mmdet/models/detectors/base.py:172\u001b[0m, in \u001b[0;36mBaseDetector.forward\u001b[0;34m(self, img, img_metas, return_loss, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39monnx_export(img[\u001b[39m0\u001b[39m], img_metas[\u001b[39m0\u001b[39m])\n\u001b[1;32m    171\u001b[0m \u001b[39mif\u001b[39;00m return_loss:\n\u001b[0;32m--> 172\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_train(img, img_metas, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    173\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    174\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_test(img, img_metas, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/tykim/scratch/lightweight/mmcv_phone/mmdetection/mmdet/models/detectors/two_stage.py:135\u001b[0m, in \u001b[0;36mTwoStageDetector.forward_train\u001b[0;34m(self, img, img_metas, gt_bboxes, gt_labels, gt_bboxes_ignore, gt_masks, proposals, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwith_rpn:\n\u001b[1;32m    133\u001b[0m     proposal_cfg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_cfg\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mrpn_proposal\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    134\u001b[0m                                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_cfg\u001b[39m.\u001b[39mrpn)\n\u001b[0;32m--> 135\u001b[0m     rpn_losses, proposal_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrpn_head\u001b[39m.\u001b[39;49mforward_train(\n\u001b[1;32m    136\u001b[0m         x,\n\u001b[1;32m    137\u001b[0m         img_metas,\n\u001b[1;32m    138\u001b[0m         gt_bboxes,\n\u001b[1;32m    139\u001b[0m         gt_labels\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    140\u001b[0m         gt_bboxes_ignore\u001b[39m=\u001b[39;49mgt_bboxes_ignore,\n\u001b[1;32m    141\u001b[0m         proposal_cfg\u001b[39m=\u001b[39;49mproposal_cfg,\n\u001b[1;32m    142\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    143\u001b[0m     losses\u001b[39m.\u001b[39mupdate(rpn_losses)\n\u001b[1;32m    144\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/tykim/scratch/lightweight/mmcv_phone/mmdetection/mmdet/models/dense_heads/base_dense_head.py:335\u001b[0m, in \u001b[0;36mBaseDenseHead.forward_train\u001b[0;34m(self, x, img_metas, gt_bboxes, gt_labels, gt_bboxes_ignore, proposal_cfg, **kwargs)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    334\u001b[0m     loss_inputs \u001b[39m=\u001b[39m outs \u001b[39m+\u001b[39m (gt_bboxes, gt_labels, img_metas)\n\u001b[0;32m--> 335\u001b[0m losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss(\u001b[39m*\u001b[39;49mloss_inputs, gt_bboxes_ignore\u001b[39m=\u001b[39;49mgt_bboxes_ignore)\n\u001b[1;32m    336\u001b[0m \u001b[39mif\u001b[39;00m proposal_cfg \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    337\u001b[0m     \u001b[39mreturn\u001b[39;00m losses\n",
      "File \u001b[0;32m~/tykim/scratch/lightweight/mmcv_phone/mmdetection/mmdet/models/dense_heads/rpn_head.py:93\u001b[0m, in \u001b[0;36mRPNHead.loss\u001b[0;34m(self, cls_scores, bbox_preds, gt_bboxes, img_metas, gt_bboxes_ignore)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloss\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[1;32m     71\u001b[0m          cls_scores,\n\u001b[1;32m     72\u001b[0m          bbox_preds,\n\u001b[1;32m     73\u001b[0m          gt_bboxes,\n\u001b[1;32m     74\u001b[0m          img_metas,\n\u001b[1;32m     75\u001b[0m          gt_bboxes_ignore\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     76\u001b[0m     \u001b[39m\"\"\"Compute losses of the head.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \n\u001b[1;32m     78\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39m        dict[str, Tensor]: A dictionary of loss components.\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m     losses \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m(RPNHead, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mloss(\n\u001b[1;32m     94\u001b[0m         cls_scores,\n\u001b[1;32m     95\u001b[0m         bbox_preds,\n\u001b[1;32m     96\u001b[0m         gt_bboxes,\n\u001b[1;32m     97\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     98\u001b[0m         img_metas,\n\u001b[1;32m     99\u001b[0m         gt_bboxes_ignore\u001b[39m=\u001b[39;49mgt_bboxes_ignore)\n\u001b[1;32m    100\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mdict\u001b[39m(\n\u001b[1;32m    101\u001b[0m         loss_rpn_cls\u001b[39m=\u001b[39mlosses[\u001b[39m'\u001b[39m\u001b[39mloss_cls\u001b[39m\u001b[39m'\u001b[39m], loss_rpn_bbox\u001b[39m=\u001b[39mlosses[\u001b[39m'\u001b[39m\u001b[39mloss_bbox\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/mm/lib/python3.8/site-packages/mmcv/runner/fp16_utils.py:205\u001b[0m, in \u001b[0;36mforce_fp32.<locals>.force_fp32_wrapper.<locals>.new_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m@force_fp32 can only be used to decorate the \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    203\u001b[0m                     \u001b[39m'\u001b[39m\u001b[39mmethod of nn.Module\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    204\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mhasattr\u001b[39m(args[\u001b[39m0\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mfp16_enabled\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m args[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mfp16_enabled):\n\u001b[0;32m--> 205\u001b[0m     \u001b[39mreturn\u001b[39;00m old_func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    206\u001b[0m \u001b[39m# get the arg spec of the decorated method\u001b[39;00m\n\u001b[1;32m    207\u001b[0m args_info \u001b[39m=\u001b[39m getfullargspec(old_func)\n",
      "File \u001b[0;32m~/tykim/scratch/lightweight/mmcv_phone/mmdetection/mmdet/models/dense_heads/anchor_head.py:483\u001b[0m, in \u001b[0;36mAnchorHead.loss\u001b[0;34m(self, cls_scores, bbox_preds, gt_bboxes, gt_labels, img_metas, gt_bboxes_ignore)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(featmap_sizes) \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprior_generator\u001b[39m.\u001b[39mnum_levels\n\u001b[1;32m    481\u001b[0m device \u001b[39m=\u001b[39m cls_scores[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdevice\n\u001b[0;32m--> 483\u001b[0m anchor_list, valid_flag_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_anchors(\n\u001b[1;32m    484\u001b[0m     featmap_sizes, img_metas, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m    485\u001b[0m label_channels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls_out_channels \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_sigmoid_cls \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n\u001b[1;32m    486\u001b[0m cls_reg_targets \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_targets(\n\u001b[1;32m    487\u001b[0m     anchor_list,\n\u001b[1;32m    488\u001b[0m     valid_flag_list,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    492\u001b[0m     gt_labels_list\u001b[39m=\u001b[39mgt_labels,\n\u001b[1;32m    493\u001b[0m     label_channels\u001b[39m=\u001b[39mlabel_channels)\n",
      "File \u001b[0;32m~/tykim/scratch/lightweight/mmcv_phone/mmdetection/mmdet/models/dense_heads/anchor_head.py:188\u001b[0m, in \u001b[0;36mAnchorHead.get_anchors\u001b[0;34m(self, featmap_sizes, img_metas, device)\u001b[0m\n\u001b[1;32m    184\u001b[0m num_imgs \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(img_metas)\n\u001b[1;32m    186\u001b[0m \u001b[39m# since feature map sizes of all images are the same, we only compute\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[39m# anchors for one time\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m multi_level_anchors \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprior_generator\u001b[39m.\u001b[39;49mgrid_priors(\n\u001b[1;32m    189\u001b[0m     featmap_sizes, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m    190\u001b[0m anchor_list \u001b[39m=\u001b[39m [multi_level_anchors \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_imgs)]\n\u001b[1;32m    192\u001b[0m \u001b[39m# for each image, we compute valid flags of multi level anchors\u001b[39;00m\n",
      "File \u001b[0;32m~/tykim/scratch/lightweight/mmcv_phone/mmdetection/mmdet/core/anchor/anchor_generator.py:236\u001b[0m, in \u001b[0;36mAnchorGenerator.grid_priors\u001b[0;34m(self, featmap_sizes, dtype, device)\u001b[0m\n\u001b[1;32m    234\u001b[0m multi_level_anchors \u001b[39m=\u001b[39m []\n\u001b[1;32m    235\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_levels):\n\u001b[0;32m--> 236\u001b[0m     anchors \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msingle_level_grid_priors(\n\u001b[1;32m    237\u001b[0m         featmap_sizes[i], level_idx\u001b[39m=\u001b[39;49mi, dtype\u001b[39m=\u001b[39;49mdtype, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m    238\u001b[0m     multi_level_anchors\u001b[39m.\u001b[39mappend(anchors)\n\u001b[1;32m    239\u001b[0m \u001b[39mreturn\u001b[39;00m multi_level_anchors\n",
      "File \u001b[0;32m~/tykim/scratch/lightweight/mmcv_phone/mmdetection/mmdet/core/anchor/anchor_generator.py:263\u001b[0m, in \u001b[0;36mAnchorGenerator.single_level_grid_priors\u001b[0;34m(self, featmap_size, level_idx, dtype, device)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msingle_level_grid_priors\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[1;32m    242\u001b[0m                              featmap_size,\n\u001b[1;32m    243\u001b[0m                              level_idx,\n\u001b[1;32m    244\u001b[0m                              dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32,\n\u001b[1;32m    245\u001b[0m                              device\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    246\u001b[0m     \u001b[39m\"\"\"Generate grid anchors of a single level.\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \n\u001b[1;32m    248\u001b[0m \u001b[39m    Note:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[39m        torch.Tensor: Anchors in the overall feature maps.\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m     base_anchors \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_anchors[level_idx]\u001b[39m.\u001b[39;49mto(device)\u001b[39m.\u001b[39mto(dtype)\n\u001b[1;32m    264\u001b[0m     feat_h, feat_w \u001b[39m=\u001b[39m featmap_size\n\u001b[1;32m    265\u001b[0m     stride_w, stride_h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrides[level_idx]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered"
     ]
    }
   ],
   "source": [
    "train_detector(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdet.apis import set_random_seed\n",
    "\n",
    "cfg.data_root = '/home/aiteam/tykim/scratch/lightweight/mmcv_phone/hold_smartphone'\n",
    "# timm이 pretraiend를 로드할 것이므로\n",
    "cfg.load_from = None\n",
    "cfg.work_dir = '/home/aiteam/tykim/scratch/lightweight/mmdet_timm/edgenext_exps'\n",
    "\n",
    "cfg.data.samples_per_gpu=16\n",
    "cfg.data.workers_per_gpu=4\n",
    "\n",
    "cfg.optimizer.lr = 0.02 * 1\n",
    "cfg.lr_config.warmup = None\n",
    "cfg.log_config.interval = 10\n",
    "\n",
    "# We can set the evaluation interval to reduce the evaluation times\n",
    "cfg.evaluation.interval = 10\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 10\n",
    "\n",
    "cfg.runner.max_epochs = 300\n",
    "\n",
    "cfg.seed = 0\n",
    "set_random_seed(0, deterministic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump_file = \"/home/aiteam/tykim/scratch/lightweight/mmdet_timm/edgenext_exps/faster_rcnn_edgenext_phone.py\"\n",
    "# cfg.dump(dump_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4b67475607a50059686a8eb20b37a22aaf5a00707f6dfe4536f1620cd9c3e846"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
